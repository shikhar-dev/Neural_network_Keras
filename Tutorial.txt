# The problem  requires us to build a Neural Network which could differentiate between sonar data obtained after refection from mines from data obtained after reflection from rocks.
The Dataset is divided into 208 examples. Each example consists of input vector X and output Y.
X is 1x60 vector and Y has value equal to either 'M' or 'R' (Mines and Rocks respectively). Thus there are 60 features hence size of input Layer will be 60.
We will be training a 2 Layered Neural Network for the task. This is a standard Problem hence accuracy will be above 70%.

# Lets start with the Line by line analysis.
# [4-13] - These Lines are importing various dependencies and I will be explaining each dependency when it will be used.

# [16-26] - Will be explained later. We will follow the flow of program.

# [29-31] - We initialize random.seed in numpy Libarary with the value of '7'(as seed = 7 [30]). This value of seed will be used in multiple places that is why we have stored it in a variable 'seed'.
            The reason for keeping value of seed = '7/constant value' is that every time we run the program, we will get same set of random numbers and thus same results which will be helpful in debugging.

# [33-35] - We now load the dataset from the file 'sonar.csv'. In the file sonar.csv the data is present in the form of a matrix with number of rows equal to the examples and each column representing a feature, plus the last column as output.
            We use pandas Library for this function as it easily handles string and is better than numpy in case of loading data.
            Now pd.read_csv(filename,header) reads the 'sonar.csv' file and returns a 'dataframe'. A datafram is a 2-d table with proper column names and row numbering.
            Thus the dataset is now stored in a proper table, this is called a dataframe. Header = none is done because we don't want to give any column names otherwise we could have done so -> https://chrisalbon.com/python/pandas_dataframe_importing_csv.html.
            'dataframe.values' just reads the data from row 1 column 1, row wise ie all the dataframe except the column names and row indexes is stored in variable 'dataset'.
            Feel free to print these variables to get a better understanding.

# [37-39] - This is general spilliting of a 2-d matrix. 'dataset[:,0:60].astype(float)' takes all rows and all columns except the last column (Output column) and casts into float and stores in variable 'X'.
            Similarly you can guess how 'Y' is filled.

# [41-44] - LabelEncoder class is imported from scikit-learn.preprocessing. The only reason to do this is that 'Y' is a vector in from of 'M's and 'R's but we want it to be in '0's and '1's.
            Here the LabelEncoder helps. First we instantiate LabelEncoder class and then use its '.fit()' method. 'fit(y)' takes all values in the vector, sorts them and maps the sorted value from '0' to 'size of vector -1'.
            After the mapping is done, we meed to convert the vector 'Y' using this mapping, this is exactly what '.transform()' method does. Thus to use transform method we have to use fit method first.
            Now the vector is converted into vrctor of '0s' and '1s'.

# [46-49] - These lines are little typical and require some prerequisites. Don't worry :) just pay attention. So first we not only want to train the model but also test its accuracy. Thus we want to train the model and cross validate it on some new test cases. Here as you can see two steps are involved, first is model generation and second is cross validation score evalutaion.
            Model Generation can be done using Keras Library (explained Later) and cross validation can be done using scikit-learn or sklearn Library (cross_val_score). Now understand that Neural Network Model is generated by Keras(Sequential) but has to be used sklearn (cross_val_score), for this to happen we need a Keras wrapper class so that the model can be used with sklearn Library.
            The wrapper class is 'KerasClassifier' [49]. All you need to do is pass a fucntion that outputs the model, number of iteration (epochs) and batch size to this class and it will do the rest. Line [49]. The fucntion passed is 'create_baseline'. So Now first understand 'create_baseline' funtion -
            [to be continued].

# [18-26] - Here we define our fucntion that will create the model and return it. We name this fucntion as 'crate_baseline()'.
            [20-23] - First we use 'Sequential' class from Keras Library to create the model. Sequentail is a very cool class and considers Neural Network Model to be a stack of sequentail Layers. So all we need to do is instantiate this class into a variable ('model') and after that keep adding on Layers.
                      We here use 'Dense' Layer, there are many more layers for eg. Droupout, but we use Dense because it is fully connected Layer, meaning all units of previous Layer ae connected to every unit of next Layer.
                      'Dense' has many arguments but the relevant ones are , 'number of units ' 60 in first Layer, 'kernel_initializer' which describes how the initial weights or kernels will be initialized and the most important 'activation', we can use variuos activation functions such as 'relu', 'sigmoid', 'tanh' etc ;
                      First we create a layer using Dense and then add it using 'model.add'. KEEP IN MIND input layer is not a layer, we add layers staring from the first Hidden Layer that is why we need to tell the first hidden Layer about input_dimension or features ('input_dim').
                      Thus we add three Layers and last one has 'sigmoid' function as activation. You can note that we do not need to give input dimensions in all the layers, as they are sequential and aware of the previous layer.
                      Lastly we compile this model and specify 'loss function or the objective function' as binary_crossentropy popularly known as logarithmic loss function. Also we specify optimization algorithm as 'adam' optimization algorithm, we could use 'SGD' or gradient descent here.
                      Metrics is assigned to accuracy telling that you want to judge the model based on accuracy.

# [46-49] - Lets get back to where we left. We now understand what KerasClassifier does and how and also its arguments. But there is still the 'StandardScaler' class from sklearn.preprocessing that we do not understand. This is not necessary to use but increases accuracy of model by aprox 5%. Basically what it does is standardizes the data (like Normalization) and we know on Standrdized data we get better accuracy.
            StandardScaler subtracts means and from evry value and divides by standard deviation.
            There is still one part left and that is 'Pipeline' from sklearn.preprocessing. For better understanding of this we have to completely understand what 'StratifiedKFold' does. But basically what we want is split out test data into parts and for each part we first standardized the dat of that part and then train our model on that part.
            Training is done by 'KerasClassifier' and standardization is done 'StandardScaler'. This two operations have to be performed on each data part.
            Thus only 'KerasClassifier' won't do the job, we need to couple it with 'StandardScaler'. This is what 'Pipeline' does. It takes a list of activities and combines them.

# [51-52] - We combine tasks in 'est' list and pass this list to 'Pipeline' and get variable 'pipeline' which does 2 operations on data, we already know about.

# [54-55] - 'StratifiedKFold' splits the dataset into 'K' parts and we are expected to train the model on each part excpet the last part, on which we test the model to get the accuracy. This procedure is performed 'K' times taking differrent parts for evalutaion and trainig if 'shuffle = True'.
            The first argument is number of splits of dataset ie 'K', next is shuffling of dataset using the 'seed' which is the third argument.

# [57-60] - Finally the last part, where the actual processing takes place. The 'cross_val_score' takes the actual 'fucntion' or 'pipeline' that return the model, it take the data set X,Y as asrgument and also 'cv = kfold' which difines the cros validation spillting technique.
            Thus it spilts data into 10 parts, standardizes the data for each parts, tains the model for each part excpet the last one, calculated the accuracy on the last part, then shuffles the data set and repeats this '10' times in total.
            The averaged accuracy is shown as output along with standard deviation.

I have written this tutorial to the best of my knowledge, but it will contain some mistakes. I request the reader to kindly contribute to correct those mistakes.
Feel free to fork and add/edit the parts which u think may make the understanding easier. Help is much Appreciated :)...